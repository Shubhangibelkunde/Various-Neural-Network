{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNJyJzhfYFBuvFy2FuknyFW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"L-f6fP_vufLc"},"outputs":[],"source":["#1. Describe the basic structure of a Feedforward Neural Network (FNN). What is the purpose of the activation function?"]},{"cell_type":"code","source":["A Feedforward Neural Network (FNN) consists of an input layer, one or more hidden layers, and an output layer. Each layer is made up of neurons (units) that are fully connected to the neurons of the next layer.\n","\n","- Input Layer: Receives the input data.\n","- Hidden Layers: Perform computations and transformations on the input data. The number of hidden layers and neurons can vary based on the complexity of the problem.\n","- Output Layer:  Produces the final output of the network, which can be in the form of classifications, predictions, etc.\n","\n","activation function is applied to the output of each neuron after a weighted sum of inputs. Its purpose is to introduce non-linearity into the model, enabling the network to learn complex patterns and make decisions beyond simple linear relationships.\n","Common activation functions include ReLU, sigmoid, and tanh."],"metadata":{"id":"I1t4GsvHuqOj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#2. Explain the role of convolutional layers in a CNN. Why are pooling layers commonly used, and what do they achieve"],"metadata":{"id":"RSNw6Q63uqLC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["In a Convolutional Neural Network (CNN), **convolutional layers** play a crucial role in feature extraction.\n","They apply convolution operations to the input data (e.g., images) using filters (kernels) that slide across the input to detect local patterns, such as edges, textures, and shapes.\n"," By learning different filter weights during training, convolutional layers can capture hierarchical features at varying levels of abstraction.\n","\n","Pooling layers are commonly used after convolutional layers to downsample the feature maps. Their main purposes are:\n","\n","1. Dimensionality Reduction: Pooling reduces the size of the feature maps, lowering the computational load and memory usage.\n","2. Translation Invariance: By summarizing the features in a region, pooling helps make the model more robust to translations and small variations in the input.\n","3. Feature Extraction Enhancement: Pooling emphasizes dominant features while discarding less important information, aiding in the generalization capability of the model.\n","\n","Common pooling methods include max pooling and average pooling."],"metadata":{"id":"FGI-6p4uuqH_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#3 What is the key characteristic that differentiates Recurrent Neural Networks (RNNs) from other neural networks? How does an RNN handle sequential data"],"metadata":{"id":"-T0AbQ4svMJj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["The key characteristic that differentiates Recurrent Neural Networks (RNNs) from other neural networks is their ability to maintain\n"," a **memory of previous inputs** through the use of recurrent connections.\n"," In contrast to feedforward networks, which process inputs independently, RNNs allow information to persist by passing the hidden state\n"," at the previous time step to the current time step.\n","\n","### Handling Sequential Data:\n","RNNs handle sequential data by taking sequences of inputs (e.g., time-series data, sentences) one element at a time,\n","while maintaining a hidden state that captures information about all prior inputs in the sequence. This allows RNNs to:\n","\n","1. **Process Input Sequences:** As each element of the sequence is fed to the network, the hidden state is updated,\n"," thus integrating information from all preceding elements.\n","2. **Capture Temporal Dependencies:** RNNs can learn and remember patterns across time, making them suitable for tasks where context and order matter,\n"," such as language modeling, speech recognition, and time series forecasting.\n","\n","Despite their advantages, traditional RNNs can struggle with long-term dependencies due to issues like vanishing gradients;\n"," this limitation has led to the development of more advanced architectures like Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRUs)."],"metadata":{"id":"PdpcdQ5yvMGJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#4 . Discuss the components of a Long Short-Term Memory (LSTM) network. How does it address the vanishing gradient problem"],"metadata":{"id":"qyUfgvrEvMDh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Long Short-Term Memory (LSTM) networks are an advanced type of Recurrent Neural Network (RNN) designed to effectively learn and remember from long sequences of data.\n"," The key components of an LSTM network include:\n","\n","1. **Cell State:** A memory cell that stores long-term information. It runs through the entire sequence and is regulated by gates, allowing it to carry forward important information.\n","\n","2. **Gates:** LSTMs have three primary gates that control the flow of information:\n","   - **Forget Gate:** Decides what information from the cell state should be discarded (forgotten). It uses a sigmoid activation to output values between 0 and 1 for each aspect of the cell state.\n","   - **Input Gate:** Determines what new information should be added to the cell state. It also uses a sigmoid function to decide which values to update and creates a vector of new candidate values to store in the cell state.\n","   - **Output Gate:** Controls what information from the cell state should be output to the next layer or the next time step.\n","    This gate uses the current input and the hidden state to determine the output value.\n","\n","### Addressing the Vanishing Gradient Problem:\n","LSTMs address the vanishing gradient problem through their unique architecture, primarily by maintaining a separate cell state that runs through the entire sequence with minimal changes.\n","The gates enable the network to learn when to retain or forget information, thereby allowing gradients to flow more easily back through time. This means that even when sequences are long,\n","LSTMs can preserve the gradient's value across many time steps, facilitating the learning of long-range dependencies effectively."],"metadata":{"id":"aPSXkoblwvHg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#5 Describe the roles of the generator and discriminator in a Generative Adversarial Network (GAN). What is the training objective for each."],"metadata":{"id":"zMx855W_wu7-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["In a Generative Adversarial Network (GAN), two main components are involved: the **generator** and the **discriminator**. Each has distinct roles and training objectives.\n","\n","### Roles:\n","1. **Generator:**\n","   - The generator's role is to create new data samples that resemble the training data. It takes random noise (latent vectors) as input and transforms this noise into synthetic data (e.g., images).\n","   - Its goal is to produce outputs that are indistinguishable from real data.\n","\n","2. **Discriminator:**\n","   - The discriminator's role is to evaluate data samples and determine whether they are real (from the true dataset) or fake (generated by the generator).\n","   - It serves as a binary classifier, outputting a probability score indicating the likelihood that the given input is real.\n","\n","### Training Objectives:\n","1. **Generator's Objective:**\n","   - The generator aims to maximize the likelihood of the discriminator making a mistake. In other words, it tries to produce data that the discriminator classifies as real.\n","    Formally, this can be represented as minimizing the negative log probability of the discriminator's output:\n","     \\[\n","     \\text{Loss}_{\\text{generator}} = -\\log(D(G(z)))\n","     \\]\n","   where \\( D \\) is the discriminator and \\( G(z) \\) is the generated data.\n","\n","2. **Discriminator's Objective:**\n","   - The discriminator seeks to correctly classify real and fake data. Its objective is to maximize its ability to distinguish between real and generated samples.\n","     This can be represented as minimizing the log probability of its correct predictions:\n","     \\[\n","     \\text{Loss}_{\\text{discriminator}} = -[\\log(D(x)) + \\log(1 - D(G(z)))]\n","     \\]\n","   where \\( x \\) is the real data and \\( G(z) \\) is the generated data.\n","\n","### Summary:\n","In summary, the generator and discriminator engage in a game where the generator aims to outsmart the discriminator by producing realistic samples,\n"," while the discriminator strives to improve its accuracy in detecting fake samples. This adversarial training process leads to the generator producing increasingly realistic outputs over time."],"metadata":{"id":"67AX7ZN3wuyK"},"execution_count":null,"outputs":[]}]}